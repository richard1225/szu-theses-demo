\section{top-k\ 空间文本簇检索系统设计}

空间文本簇检索过程包括文本预处理、空间索引构建、文本相似性度量、聚类算法选择、结果评价等过程。文本预处理阶段分为文本分词、构建倒排索引表；空间索引结构采用IR-树结构；文本相似度计算为余弦相似度；聚类算法选择基于密度的聚类方法中最具代表性的DBSCAN算法。总体设计流程如图\ref{topksearchprocess}所示：
\begin{figure}[htbp]
	% caption放上面就会显示在图的上方，出现在下面就是出现在图的下方
	% label的位置也有讲究
	\begin{center}
		\includegraphics[width=5in]{process-of-design-topk.png}
		\caption{top-k空间文本簇检索设计流程}
		\label{topksearchprocess}
	\end{center}
\end{figure}

\subsection{top-k\ 空间文本簇查询定义}

定义一个数据集$D$，$p<\lambda,\varphi>\in D$,其中$p . \lambda$表示数据对象的位置信息，$p . \varphi$表示p的文本描述或文档信息（例如，餐馆的设施和菜单）。文档信息$p . \varphi$由一个向量$(w1, w2，...，wi)$表示，其中每个维度对应于文件中一个不同的词项$ti$。向量中每一项的权值$wi$用tf-idf值表示。词项$t$在文档$p$中出现的次数称为词项频率，记为$tf_{t,p}$。为了避免每个词项对于文档都同样重要（即对于文档的分类的贡献率相同），因此引入一个新的因子文档频率$df_{t}$,表示出现$t$的所有文档的数目。由于df本身往往较大，因此实际运用中需要映射到较小的取值上。为此，设定文档集总数为$N$，词项t的逆文档频率idf的定义如公式(\ref{idf})

\begin{equation}
	\label{idf}
	idf_{t} = log\frac{N}{df_{t}}
\end{equation}
对于一篇文档中的每个词项，将tf和idf组合形成最终的权重。tf-idf权重的最终计算公式如下：
\begin{equation}
	\label{tfidf}
	tfidf_{t,d} = tf_{t,d} \times idf_{t}
\end{equation}

\newpage

下面对于top-k空间文本簇给出相关扩展定义：

\begin{enumerate}[labelwidth=1.5cm,labelindent=10pt,leftmargin=2.2cm,label=\bfseries 定义 \arabic*:,align=left]
  \setcounter{enumi}{0}% just for the example
  \item 给定关键字集合\varphi和相关对象集合$D\varphi$,有(i) $D\varphi \in D$且(ii) $\forall p \in D\varphi$($\varphi \cap p. \varphi\neq\phi$).
  \item 有相关对象$p \in D_{\varphi}$，其$\varepsilon$-邻近点集合表示为$N\varepsilon(p)$，有$N\varepsilon(p) = { p_{i}\in D\varphi \mid \left| pp_{i}\right| < \varepsilon }$
  \item 相关对象$p$的$ \varepsilon $-邻近点集合$N \varepsilon (p)$如果是密集型的则表示该集合至少包含$minpts$个对象，即$\left| N \varepsilon (p) \right| \geq minpts$
  \item 如果相关对象$p$的$\varepsilon$-邻近点集合$N_{\varepsilon}(p)$是密集型的，则$p$为核心对象。
  \item 如果一组相关对象$p_{i}$ 和$p_{j}$是可直达的，则有: 
  \begin{enumerate}
		\item $p_{i} \in  N_{\varepsilon}(p_{j})$，且
		\item \left| N_{\varepsilon}(p) \right| \geq minpts
	\end{enumerate}
	
	\item 有一组可直达对象$p_{i}$和$p_{j}$以及一串相关对象$p_{1}, ..., p_{n}$，如果有$p_{i} = p_{1}, p_{j} = p_{n}$,则有$p_{m}和p_{m}+1$是可直达的，其中$1 \leq m < n.$ 

	\item 如果相关对象$p_{i}$和$p_{m}$可直达，相关对象$p_{j}$和$p_{m}$可直达，则称对象$p_{i}$和$p_{j}$可连接

	\item 一个空间文本聚类R同时满足: 
	\begin{enumerate}
		\item $R \subseteq D_{\varphi}$，且
		\item $R$是最大化集合，即当只考虑$D_{\varphi}$中的对象时，通过$\varepsilon$-邻近点集合$\forall p_{i},p_{j} \in R$满足$p_{i},p_{j}$是可连接的
	\end{enumerate}

\end{enumerate}

一个空间文本簇从数据集D中与关键词$\varphi$相关的对象集$D_{\varphi}$中找到的基于密度的簇。top-k空间文本簇(k-STC)查询$q =<\lambda, \varphi, k, \varepsilon, minpts>$有五个参数：$\lambda$表示点（用户）位置， $\varphi$表示一组关键词，$k$表示请求返回簇的数量，$\varepsilon$表示在邻近区域的距离约束，minpts表示密集型$\varepsilon$-邻域的密度阙值，即最少对象数量。k-STC将返回一个包含$k$个空间文本簇的列表，这些簇将由计分函数进行打分，并根据得分升序排列。基于密度的聚类算法使得每个簇都将最大化，意味着前$k$个簇不会重叠。密度约束参数$\varepsilon$, $minpts$依赖于用户的喜好，表明用户能够接受的移动范围。

直观地说,一个高文本相关性且位于查询位置附近的簇对应一个高排名的结果。簇$R$的计分函数如下：

\begin{equation}
	\label{evaluate}
	Scoreq(R) = \alpha\cdot d_{q.\lambda}(R)+(1-\alpha)\cdot (1-tr_{q.\varphi}(R))
\end{equation}

其中，$d_{q.\lambda}(R)$是查询位置$\lambda$与$R$中的对象之间的最小空间距离，本论文采用点在二维空间下欧式距离，即点$a = (x1, y1)$和$b = (x2, y2)$的距离为：

\begin{equation}
	\label{distance}
	d(a,b) = \sqrt{(x1-x2)^2 + (y1 - y2)^2};
\end{equation}

\newpage
$tr_{q.\varphi}(R)$是$R$中的最大文本相关性，$tr_(q.\psi)(R)=\sum_(t \in q.\psi \cap R.\psi)w_t $，相似度计算采用余弦相似度（cosine similarity），即对于文档$p1 = \vec{V}(p_1 ) = (w_1, w_2, ..., w_i), p_2 = \vec{V}(p_2) = (w_1, w_2, ..., w_i)$, 文档$p_1, p_2$的余弦相似度为：

\begin{equation}
	\label{simp1p2}
	sim(p_1, p_2) = \frac{\vec{V}(p_1) \cdot \vec{V}(p_2)}{\left| \vec{V}(p_1) \right| \cdot \left| \vec{V}(p_2) \right|} 
\end{equation}

分子为$\vec{V}(d_1)$和$\vec{V}(d_2)$的内积（inner product），分母为$\vec{V}(d_1)$和$\vec{V}(d_2)$的欧几里得长度的乘积。参数$\alpha$用于平衡簇的空间邻近性和文本相关性。


\subsection{top-k\ 空间文本簇检索系统架构}
本论文的空间文本簇检索系统架构按模块在大方向上分为数据处理模块和检索模块，数据处理模块又细分为分词模块、文本向量化模块、构建空间索引模块，分词模块调用了两个外部接口。分词模块和文本向量化模块将数据对象的文本信息转化为结构化信息，和由数据对象的空间信息构成的空间索引结构R-树一起构成空间文本索引结构IR-树。检索模块依据空间关键字在IR-树上查询邻域内相关对象，查找满足条件的簇。总体的系统架构图如图\ref{system_structure}所示：

\begin{figure}[htbp]
	% caption放上面就会显示在图的上方，出现在下面就是出现在图的下方
	% label的位置也有讲究
	\begin{center}
		\includegraphics[width=6in]{structure-of-topk.png}
		\caption{top-k空间文本簇系统框架图}
		\label{system_structure}
	\end{center}
\end{figure}

\subsubsection{数据处理模块}

网络文本通常呈现出非结构化，不能被计算机直接处理。所以聚类首先需要将收集到的网络文本转换为结构化的数字信息，即空间文本表示模型。本论文采用向量空间模型（vector space model，简称VSM），向量空间模型是信息检索的基础，包括文档分类和聚类等，是目前数据挖掘中最常用的表示模型。另外常用的表示模型还有布尔模型和概率模型等。

数据处理模块通过文本分析将数据对象的文本描述信息传递给分词模块处理，分词模块根据中英文作出相应的分词处理；文本向量化模块则计算出分词后每个词项在文本中的权重，并构建倒排索引表。完成对单个文本的向量化处理后，构建向量空间模型，称为反向文件。最后将空间字段和反向文件传递给空间文本构建模块，建立IR-树。下面对每个小模块作进一步说明：

\textbf{1.分词模块:} 文本中，英文的表示以单词作为基本单位，单词之间以空格作为分隔符。根据分隔符划分文本并去除停用词后得到的词项结构已经可以达到了不错的分词效果。但是英文文本通常涉及到一个单词的多种形式,比如organize、organizes和organizing。还有表示相同意义的同源词，比如democracy、democratic和democratization。检索一个关键词时，对其同源词的检索会让结果更加有意义。而简单的利用标点和空格划分会使得单词的复现率高。词干（stem）还原[38]可以减少了单词的变化形式，或将派生词统一为基本形式。比如：

am, are, is => be

car, cars, car’s, cars’=> car

最常用的词干还原算法是Porter算法，算法复杂但是高效。值得注意的是，文本分词只是将自然语言结构化，词干还原方法使得检索更加全面但是并不会显著改善英文检索性能。

另一方面，对于中文分词，情况要复杂的多。中文没有直接的分隔符，灵活性和复杂性要比英文高得多。目前主要有三种中文分词方式：（1）基于统计的分词方法；（2）基于字符串匹配的分词方法（基于词典的方法）；（3）基于理解的分词方法（基于规则的方法）。本研究将使用github上一种基于统计分词方法的开源中文分词项目jieba分词，对中文文本进行分词处理。 

分词模块将数据集的每一个文档按照文本字段和空间字段分别处理，文本字段即对象的描述内容，空间字段即对象的位置信息。分词模块会对分开处理中文文本和英文文本，对于中文文本，模块调用jieba分词接口进行分词处理，英文文本根据分隔符对单词进行预处理，再调用词干还原接口对单词作进一步处理。分词结果以词项列表的形式存储。通过接口处理完文本后，每个文本的词项都存储在内存中，并以传递给文本向量化模块。

\textbf{2.文本向量化模块:} 文本向量化模块根据每个文本的分词结果计算出每个词项的tf-idf值，以向量形式表示文本。首先，统计出一个词项词典d，该词典包含数据集D全部的词项t。然后，统计出每个词项对应tf和df值。根据公式（2-1）〖idf〗_(t )=log N/〖df〗_t   和（2-2）tf-idft,d = tft,d × idft计算出tf-idf值。至此，文档p.φ可以向量化表示为(w1, w2，···，wi)，w_i=tf_i×idf_i。
% ------------sim(p_1, p_2) = \vec{p_1}---------------------------------- \cdot \vec{p_2}
% ----------------------------------------------
BPR是一个应对隐式反馈很流行的推荐框架. 它基于这样一个偏好假设: 如果一个用户$u$已经选择了物品$i$但是没有选择物品$j$，那么在BPR中, 我们认为相对于物品$j$用户$m$更喜欢物品$i$,并定义用户$u$关于物品$i$与$j$的偏好关系为：
\begin{equation}
\label{pairwisepre}
p \left( i \succ_u j \right) := f \left( x_{uij} \right),
\end{equation}
这里$f \left(x\right) = 1/\left(1+exp\left(-x\right)\right)$\footnote{$f \left(x\right)$即为sigmoid函数}, $x_{uij} := s\left(u,i\right) - s\left(u,j\right)$, $s\left(\cdot,\cdot\right)$可以是任何表示用户与物品相关程度的函数。在BPR中, $s\left(\cdot,\cdot\right)$为用户对物品的预测值, 即$s\left(u,i\right) = \hat{r}_{ui}$, $x_{uij} = \hat{r}_{ui}-\hat{r}_{uj}$.


\subsubsection{预测公式}

在BPR中, 用户$u$对于物品$i$的预测值$\hat{r}_{ui}$公式为:
\begin{equation}
\hat{r}_{ui} = U_{u\cdot}V_{i\cdot}^T + b_i
\end{equation}


\subsubsection{Likelihood of Pairwise Preference}

伯努利分布(Bernouli distribution)是关于布尔变量 $x \in \{0,1\}$ 的概率分布, 其连续参数 $p \in \left[0,1\right]$的概率.
\begin{equation}
\left( x|p \right) = Ber\left(x|p \right)=p^x\left(1-p \right)^{1-x}
\end{equation}

若记事件 $\left(\hat{r}_{ui} > \hat{r}_{uj}\right)$ 的概率为$p\left(\hat{r}_{ui} > \hat{r}_{uj}\right)$, 布尔变量$\delta\left(\left(u,i\right) \succ \left(u,j\right)\right)$ 服从伯努利分布, 那么用户$u$的likelihood of pairwise preference 在中被定义为:
\begin{equation}
\label{LPP}
\begin{aligned}
LPP_u  
&= \prod_{i,j \ \in \  \mathcal{I}}p\left(\hat{r}_{ui} > \hat{r}_{uj}\right)^{\delta\left(\left(u,i\right) \succ \left(u,j\right)\right)} \left[1-p\left(\hat{r}_{ui} > \hat{r}_{uj}\right)\right]^{1-\delta\left(\left(u,i\right) \succ \left(u,j\right)\right)}\\
&= \prod_{\left(u,i\right) \succ \left(u,j\right)}p\left(\hat{r}_{ui} > \hat{r}_{uj}\right)\prod_{\left(u,i\right) \preceq \left(u,j\right)}\left[1-p\left(\hat{r}_{ui} > \hat{r}_{uj}\right)\right]
\end{aligned}
\end{equation}

这里的$\left(u,i\right) \succ \left(u,j\right)$ 表示用户 $u$ 相比物品 $i$ 更喜欢物品 $j$.

用 $f \left(\hat{r}_{uij} \right)$ 来近似表示概率
$p\left(\hat{r}_{ui} > \hat{r}_{uj}\right)$ , 对于公式\ref{LPP}取其对数即$\ln LPP_u$, 那么就有:
\begin{equation}
\label{eq5}
\begin{aligned}
\ln LPP_u
&= \ln \prod_{\left(u,i\right) \succ \left(u,j\right)} f \left(\hat{r}_{uij}\right) + \ln \prod_{\left(u,i\right) \preceq \left(u,j\right)}\left[1- f \left(\hat{r}_{uij}\right)\right]\\
&= \ln \prod_{\left(u,i\right) \succ \left(u,j\right)} f \left(\hat{r}_{uij}\right) + \ln \prod_{\left(u,i\right) \succ \left(u,j\right)}\left[1-\left(1- f \left(\hat{r}_{uij}\right)\right)\right]\\
&= \ln \prod_{\left(u,i\right) \succ \left(u,j\right)} f \left(\hat{r}_{uij}\right) + \ln \prod_{\left(u,i\right) \succ \left(u,j\right)} f \left(\hat{r}_{uij}\right)\\
&= 2\ln \prod_{\left(u,i\right) \succ \left(u,j\right)} f \left(\hat{r}_{uij}\right)\\
&= 2 \sum_{i\in\mathcal{I}_u^{tr}}
\sum_{j \in \mathcal{I}\setminus \mathcal{I}_u^{tr}}\ln f \left(\hat{r}_{uij}\right)
\end{aligned}
\end{equation}
在这里$\hat{r}_{uij} = \hat{r}_{ui} - \hat{r}_{uj}$, $f \left(x\right) = 1/\left(1+exp\left(-x\right)\right)$, .

\subsubsection{目标函数}

基于上面的成对偏好假设，可以从隐式反馈数据集中得到所有的偏好集合$D_S := \{\left(u,i,j\right) | v_i \in I_{u}^+ \wedge v_j \in I \setminus I_{u}^+\}$，$I_m^+$表示被用户$u$选择过的物品集合，三元组$\left(u,i,j\right)$表示用户$u$选择过物品$v_i$但是没有选择过物品$v_j$。我们把$v_i$叫做一个positive item，$v_j$叫做一个negative item。对于给定的集合$D_S$, BPR的目标便是最大化所有user-item pair的似然偏好：

\begin{equation}
\label{eq6}
arg \max_{\substack \Theta } \prod_{\left(u,i,j\right) \in D_S} p\left(i \succ_u j\right),
\end{equation}

公式\eqref{eq6}等价于最小化负的对数似然函数：

\begin{equation}
\label{Lfeedback}
L_{feedback} = - \sum_{\left(u,i,j\right) \in D_S}\ln f \left( x_{uij}\right) + \lambda\|\Theta\|^2,
\end{equation}
这里的$x_{uij} = \hat{r}_{uij}$, $\Theta$表示算法中需要学习的模型参数集合，$\lambda$表示超参数集合。在实际的算法学习中, BPR的学习算法经常采用均匀采样的随机梯度下降(\textbf{S}tochastic \textbf{G}radient \textbf{D}escent)进行迭代学习。

更为具体的,公式\eqref{Lfeedback}也就是最小化下面的目标函数(Objective Function): 
\begin{equation}
\label{eq8}
\min_{\substack\Theta}\sum_{u\in\mathcal{U}} \ \sum_{i\in\mathcal{I}_u}\sum_{j\in\mathcal{I}\setminus\mathcal{I}_u}\Phi_{uij}
\end{equation}
这里的
$\Phi_{uij}
= 
- \ln f \left(\hat{r}_{uij}\right) 
+ \frac{\alpha_u}{2}\|U_{u\cdot}\|^2
+ \frac{\alpha_v}{2}\|V_{i\cdot}\|^2
+ \frac{\alpha_v}{2}\|V_{j\cdot}\|^2
+ \frac{\beta_v}{2}\|b_{i}\|^2
+ \frac{\beta_v}{2}\|b_{j}\|^2$, $\Theta = \{U_{u\cdot},V_{i\cdot},b_i\}
$的将要学习的参数集合。


\subsubsection{随机梯度}
对于一个随机采样而得的三元组$\left(u,i,j\right)$, 对目标函数中的参数求其偏导即可得梯度。

在此之前先做一些准备工作，对于函数$f(x) = 1/\left(1+e^{-x}\right)$的导数:

\begin{equation*}
f^{'}(x) = -\frac{1}{\left(1+e^{-x}\right)^2} e^{-x}\left(-1\right) = \frac{e^{-x}}{\left(1+e^{-x}\right)^2} = \frac{1}{\left(1+e^{x}\right)\left(1+e^{-x}\right)} = {f(x)f(-x)}
\end{equation*}

下面开始对参数$U_{u\cdot}$求其偏导：
\begin{equation}
\begin{aligned}
\bigtriangledown U_{u\cdot} 
= \frac{\partial \Phi_{uij}}{\partial U_{u\cdot}}
&=-\frac{\partial \ln f\left(\hat{r}_{uij}\right)}{\partial f\left(\hat{r}_{uij}\right)} 
\frac{\partial f\left(\hat{r}_{uij}\right) }{\partial \hat{r}_{uij}} 
\frac{\partial \hat{r}_{uij}}{\partial U_{u\cdot}}
\ + \  \alpha_uU_{u\cdot}\\
&= -\frac{1}{f\left(\hat{r}_{uij}\right)} 
\frac{\partial f\left(\hat{r}_{uij}\right) }{\partial \hat{r}_{uij}} 
\frac{\partial \hat{r}_{uij}}{\partial U_{u\cdot}}
\ + \  \alpha_uU_{u\cdot}\\
&= -\frac{1}{f\left(\hat{r}_{uij}\right)} 
{f\left(\hat{r}_{uij}\right) f\left(-\hat{r}_{uij}\right)}
\frac{\partial f\left(\hat{r}_{ui} - \hat{r}_{uj}\right) }{\partial U_{u\cdot}} 
\ + \ \alpha_uU_{u\cdot}\\
&= -{f\left(-\hat{r}_{uij}\right)} \frac{\partial f\left[\left(U_{u\cdot}V_{i\cdot}^T+b_i\right) - \left(U_{u\cdot}V_{j\cdot}^T+b_j\right)\right] }{\partial U_{u\cdot}} 
\ + \ \alpha_uU_{u\cdot}\\
&= -{f\left(-\hat{r}_{uij}\right)} \left(V_{i\cdot} - V_{j\cdot}\right)  
\ + \  \alpha_uU_{u\cdot}\\
\end{aligned}
\end{equation}

同样其他参数随机梯度如下:
%\begin{equation}
\begin{align} %align环境不要有空行
\bigtriangledown V_{i\cdot} &= \frac{\partial \Phi_{uij}}{\partial V_{i\cdot}}=-f\left(-\hat{r}_{uij}\right)U_{u\cdot} + \alpha_vV_{i\cdot}\\
\bigtriangledown V_{j\cdot} &= \frac{\partial \Phi_{uij}}{\partial V_{j\cdot}}=-f\left(-\hat{r}_{uij}\right)\left(-U_{u\cdot}\right) + \alpha_vV_{j\cdot}\\
\bigtriangledown b_i        &= \frac{\partial \Phi_{uij}}{\partial b_i} =-f\left(-\hat{r}_{uij}\right)+\beta_vb_i\\
\bigtriangledown b_j        &= \frac{\partial \Phi_{uij}}{\partial b_j} =-f\left(-\hat{r}_{uij}\right)\left(-1\right)+\beta_vb_j
\end{align}
%\end{equation}

\subsubsection{迭代更新}
对于三元组  $\left(u,i,j\right)$ 在采用SGD的BPR算法中的更新公式如下:
%align环境的每行公式默认会进行编号，aligned环境不会
\begin{align}
	\label{eq10}
U_{u\cdot} &= U_{u\cdot} - \gamma\bigtriangledown U_{u\cdot}\\
V_{i\cdot} &= V_{i\cdot} - \gamma\bigtriangledown V_{i\cdot}\\
V_{j\cdot} &= V_{i\cdot} - \gamma\bigtriangledown V_{j\cdot}\\
b_{i\cdot} &= b_i - \gamma\bigtriangledown b_{i}\\
b_{j\cdot} &= b_j - \gamma\bigtriangledown b_{j}
\end{align}

这里的 $\gamma$ 为学习率(learning rate).

\subsubsection{BPR算法}
如算法\ref{al1}即为采用SGD求解的BPR算法。
\IncMargin{1em}
\begin{algorithm}[ht]
	\SetAlgoNoLine %不要算法中的竖线
	\BlankLine
	
	initialize the model parameter $\Theta$\;
	\For {$t_1 = 1,\cdots,T$}{
		\For {$t_2 = 1,\cdots, |\mathcal{P}|$}{
			Randomly pick up a pair $\left(u,v_i\right) \in \mathcal{P}$\;
			Randomly pick up an item $v_j$ from $\mathcal{I} \setminus \mathcal{I}_{u}^+$\;
			Calculate the gradients via Eq.(9-13)\;
			Update the model parameters via Eq.(14-18)\;
		}	
	}
	\caption{The SGD algorithm for BPR}
	\label{al1}%label 放置的位置有讲究， 放后面
\end{algorithm}
\DecMargin{1em}

\subsubsection{收敛缓慢的原因}
由于上面的均匀采样方式会产生很多对于参数学习贡献微弱的train pairs,因此常常会导致收敛缓慢。确切的讲，对于一个给定的训练采样$\left(u,i,j\right) \in D_S$, 由公式\ref{Lfeedback}对随机梯度下降的任意一参数$\theta \in \Theta$求其偏导:

\begin{equation}
\label{eq19}
\frac {\partial L_{feedback}} {\partial\theta} 
= -f\left(-x_{uij}\right)\frac{\partial\left(x_{uij}\right)}{\partial\theta}
= \left(f\left(x_{uij}\right)-1\right) \frac{\partial\left(x_{uij}\right)}{\partial\theta}
\end{equation}

根据公式\eqref{eq19},如果$f \left(x_{uij}\right) \rightarrow +1$,随机梯度将接近于0，则训练采样$\left(u,i,j\right)$对于优化目标的贡献将会变得很小。
\begin{figure}[htbp]
	\begin{center}
		\includegraphics[width=4in]{sigmoid}
		\caption{sigmoid函数$f \left(x\right) $图像}
		\label{gra-sigmoid}
	\end{center}
\end{figure}

联系公式\eqref{eq19}与公式\eqref{pairwisepre}，由图\ref{gra-sigmoid} sigmoid函数图像可得, 当$f \left(x_{uij}\right) \rightarrow +1$时, 也就是$x_{uij} = \hat{r}_{ui} - \hat{r}_{uj}$越来越大, 即用户对于物品$v_i$与$v_j$的预测差值越来越大. 因此为了加速学习, 针对一个已有的user-item pair中的物品 $v_i$，要采样的物品$v_j$应当是$v_i$相比有竞争力的物品, 更进一步说也就是由该用户对于$v_i$与$v_j$的偏好得分应该是相近的，否则这个采样对于SGD便是低效的采样。

从经验上来讲，每个用户只会浏览一小部分的物品并对这些浏览过的物品提供一些交互反馈。如果均匀采样器均等地从整个物品集合中采样negative  item.对于一个user-item  pair,大部分均匀采样的物品并不具有可比性或者很难被相关的用户浏览。举个例子，iPhone与牙刷或iPhone与一个冷门的手机品牌可能会经常被均匀采样器采得。而由于这些低效的training pair对于SGD几乎作用很小，整个训练过程便会收敛地极其缓慢。

除此以外，与经典的分解技术相似，如果一个用户或物品缺乏足够的反馈，其对应的隐式表达往往不能够被很好的学习到。在现实世界数据集中，用户行为与物品流行度的分布往往呈现长尾状。这就导致了大部分的用户和物品仅仅有很小部分的反馈数据。此外，在真实的推荐系统中，新的个体可能在任何时间被加入到推荐系统中。因此，BPR框架也很容易受制于冷启动问题。

\subsection{Latent Dirichlet Allocation}
Latent Dirichlet allocation(LDA)，隐含狄利克雷分布，是一种主题模型(topic model)，它可以将文档集中每篇文档的主题按照概率分布的形式给出。同时它是一种无监督学习算法，在训练时不需要手工标注的训练集，需要的仅仅是文档集以及指定主题的数量即可。此外LDA的另一个优点则是，对于每一个主题均可找出一些词语来描述它。

LDA首先由于2003年提出，目前在文本挖掘领域包括文本主题识别、文本分类以及文本相似度计算方面都有应用。


\subsubsection{数学模型}
LDA是一种典型的词袋(Bag-of-words)模型，即它认为一篇文档(document)是由一组词(word)构成的一个集合，词与词之间没有顺序以及先后的关系。一篇文档可以包含多个主题(topic)，文档中每一个词都由其中的一个主题生成。

\begin{figure}[htbp]
	% caption放上面就会显示在图的上方，出现在下面就是出现在图的下方
	% label的位置也有讲究
	\begin{center}
		\includegraphics[width=4in]{LDA}
		\caption{LDA 贝叶斯网络结构}
		\label{gra4}
	\end{center}
\end{figure}

另外，正如Beta分布是二项式分布的共轭先验概率分布，狄利克雷分布作为多项式分布的共轭先验概率分布。因此正如图\ref{gra4}, LDA贝叶斯网络结构中所描述的，在LDA模型中一篇文档生成的方式如下:
\begin{itemize}
	\item 从狄利克雷分布$\alpha$ 中取样生成文档$i$的主题分布$\theta_i$
	\item 从主题的多项式分布$\theta_i$中取样生成文档$i$第$j$个词的主题$z_{i, j}$
	\item 从狄利克雷分布$\beta $中取样生成主题$z_{i, j}$的词语分布$\phi_{z_{i, j}}$
	\item 从词语的多项式分布$\phi_{z_{i, j}}$中采样最终生成词语$w_{i, j}$
\end{itemize}
因此整个模型中所有可见变量以及隐藏变量的联合分布是
\begin{equation}
p(w_i, z_i, \theta_i, \Phi | \alpha, \beta) = \prod_{j = 1}^{N} p(\theta_i|\alpha)p(z_{i, j}|\theta_i)p(\Phi|\beta)p(w_{i, j}|\theta_{z_{i, j}})
\end{equation}


最终一篇文档的单词分布的最大似然估计可以通过将上式的$\theta_i$以及$\Phi$进行积分和对$z_i$进行求和得到
\begin{equation}
p(w_i | \alpha, \beta)  = \int_{\theta_i}\int_{\Phi }\sum_{z_i}p(w_i, z_i, \theta_i, \Phi | \alpha, \beta) 
\end{equation}


根据$p(w_i | \alpha, \beta)$ 的最大似然估计，最终可以通过吉布斯采样等方法估计出模型中的参数。


\subsubsection{使用吉布斯采样估计LDA参数}
在LDA最初提出的时候，人们使用EM算法(Expectation-maximization algorithm)进行求解，后来人们普遍开始使用较为简单的Gibbs Sampling，具体过程如下：
\begin{itemize}
	\item 首先对所有文档中的所有词遍历一遍，为其都随机分配一个主题，即$z_{m,n}=k\sim Mult(1/K)$,其中$m$表示第$m$篇文档，$n$表示文档中的第$n$个词，$k$表示主题，$K$表示主题的总数，之后将对应的$n^{\left(k\right)}_m+1$, $n_m+1$, $n^{\left(t\right)}_k+1$, $n_k+1$, 他们分别表示在$m$文档中$k$主题出现的次数，$m$文档中主题数量的和，$k$主题对应的$t$词的次数，$k$主题对应的总词数。
	\item 之后对下述操作进行重复迭代。
	\item 对所有文档中的所有词进行遍历，假如当前文档$m$的词$t$对应主题为$k$，则$n^{\left(k\right)}_m-1$, $n_m-1$, $n^{\left(t\right)}_k-1$, $n_k-1$, 即先拿出当前词，之后根据LDA中topic sample的概率分布sample出新的主题，在对应的$n^{\left(k\right)}_m$, $n_m$, $n^{\left(t\right)}_k$, $n_k$上分别$+1$。
	\begin{equation}
	p(z_i=k|z_{-i},w) \propto k(n^{(t)}_{k,-i}+\beta_t)(n_{m,-i}^{(k)}+\alpha_k)/(\sum_{t=1}^{V}n_{k,-i}^{(t)}+\beta_t)
	\end{equation}
	\item 迭代完成后输出主题--词参数矩阵$\Phi$和文档--主题矩阵$\Theta$
	\begin{align}
	\phi_{k,t}   &=(n_k^{(t)}+\beta_t)/(n_k+\beta_t)  \\
	\theta_{m,k} &=(n_m^{(k)}+\alpha_k)/(n_m+\alpha_k)
	\end{align}
\end{itemize}









\subsection{本章小结}
本章首先介绍了采用SGD求解的Bayesian Personalized Ranking(BPR)推荐算法, 并且对可能导致其收敛缓慢的均匀采样策略做了讨论。然后简要介绍了LDA模型。