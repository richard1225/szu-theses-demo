\section{文本预处理}
在自然语言处理中，数据源处理的好坏直接影响到跑出来的模型的效果，可见，文本预处理在自然语言处理中是尤为重要的一环。对于不同语言的文本，预处理的过程是不一样的。对于英文，由于词与词之间已经用空格分隔开了，要做的只是把英文单词的时态和单复数等问题解决就行。

但是对于中文文本来说，由于词与词之间不存在间隔，无法直接分词，需要借助分词工具。同样，分词的效果好坏直接影响了聚类的效果。本章主要介绍我在中文文本预处理中执行的三大操作：名词归一化替换、中文分词、word2vec词向量化。

\subsection{非专业名词归一化替换}

在一份没有经过任何处理的法律文书中，往往充斥着大量原告被告、地名、金额、日期、数量、时间、期限、法律法条等信息，这类信息若不处理，会导致下一步的中文分词分错，进而使得聚类模型跑偏，结果不准确。
但是这类信息也是有效的，单纯地去掉也会导致关键因子的丢失，故采用替换为统一的名词来处理，比如原告名字统一替换为“原告”，金额数据统一替换为“金额”等等。

本文使用了两种方法来替换原文：一种是基于词典的替换，另一种是基于正则的替换。在网络上可以找到很全的地名字典，故可以直接用地名字典去匹配原文中的地名，然后统一替换成“地址”。

剩下的非专业名词使用正则表达式来进行替换。所用正则表达式如下：
\begin{itemize}
	\item \textbf{大数目金额}：\zihao{-4.5}$(([1-9]\backslash \backslash d*[\backslash \backslash d,，]*\backslash \backslash .?\backslash \backslash d*)|(0\backslash \backslash .[0-9]+))$(元|百万|万元|亿元)

	\item \textbf{原、被告名字}: \zihao{-4.5}$((?<=($原告|被告$)$：$).*?(?=$，$|\/|$。|、))

	\item \textbf{日期}：\zihao{-4.5}$\backslash$d$\{$4$\}$(年)$\backslash$d$\{$1,2$\}$(月)$\backslash$d$\{$1,2$\}$(日)|(?:$\backslash$d$\{$1,2$\}$|[$\backslash$u4e00-$\backslash$u9fa5])(?:个月|个星期|个工作日|日)|$\backslash$d$\{$4$\}$(年)$\backslash$d$\{$1,2$\}$(月)|$\backslash$d$\{$1,2$\}$(月)$\backslash$d$\{$1,2$\}$(日)|$\backslash$d$\{$4$\}$(> >
	年)|$\backslash$d$\{$1,4$\}$(日)

	\item \textbf{利息}: \zihao{-4.5}($\backslash$d*($\backslash$.$\backslash$d*)?(?:$\backslash$\%|％|‰))|(百|千|万)分之[$\backslash$u4e00-$\backslash$u9fa5]$\{$1,2$\}$(点[$\backslash$u4e00-$\backslash$u9fa5])?|($\backslash$d(分))

	\item \textbf{数量}: \zihao{-4.5}((?:$\backslash$d|[$\backslash$u4e00-$\backslash$u9fa5])(?:份|张|次))
	
	\item \textbf{期限}: \zihao{-4.5}(?:$\backslash$d$\{$1,2$\}$|[$\backslash$u4e00-$\backslash$u9fa5])(?:个月|个星期)|[$\backslash$u4e00-$\backslash$u9fa5]月|$\backslash$d$\{$1,3$\}$(天)
\end{itemize}

通过上述正则表达式，匹配替换原词为指定的统一词汇，可以让文本的结构更加清晰，同时又保留了这部分关键词，分词的准确率有了很大的提升。

\subsection{中文分词}
中文分词，是自然语言处理中的最基本的问题，如果中文分词不正确，中文NLP的其他算法也无法进行下去。由于中文的字与词是相连的，没有一个结构化的方法来划分，故需要一些算法来把它们分成独立的词，目前常用的分词方法有五种。

第一种是\textbf{正向最大匹配}，最大正向匹配是基于词典的匹配，首先设置一个最长窗口长度。然后从句子左边开始，向右扫描，每次移动一格，若窗口内有词语出现在了词典中，则标记该词语。这种算法能在大多数情况下生效，但是一旦句子里有多义词或未登陆词\footnote{\label{unlogin_word}未登陆词：词典中未录入的词语}，那这个算法将无法识别。第二种是\textbf{逆向最大匹配}，这种算法原理跟正向最大匹配一样，只是窗口滑动方向是从最右边的词尾开始，滑向词头。第三种是\textbf{双向最大匹配}，这个算法同时运用了正向最大匹配和逆向最大匹配，得到两种匹配的结果后对比，若两种分词的结果一样，那就认为是切分成功，采用分词结果。若两种算法的分词结果不一致，则表明出现了歧义现象或者出现了未登录词语。第四种是\textbf{N-gram双向最大匹配}。Ngram算法是基于统计的分词，他是通过计算样本的所有切分方案的概率，选择概率最大的那个。这个模型是基于一个这样的假设，一个句子中，第n个词的出现只与前面n-1个词相关，与其他句子中的词不相关。设$W_i$为句子中每个词的概率，$P \left( T \right)$为当条句子的概率，如公式\ref{ngram_approx}所示，整条句子的概率，约等于这个句子中每个词的概率的乘积。第五种是\textbf{基于隐马尔科夫模型(HMM)的分词}，HMM模型主要是解决未登陆词的问题。

\begin{equation}
	\begin{aligned}
		P(T) &= P(W_1W_2W_3…W_n) \\
				&= P(W_1)P(W_2|W_1)P(W_3|W_1W_2)…P(W_n|W_1W_2…W_{n-1}) \\
				&\approx P(W_1)P(W_2|W_1)P(W_3|W_2)…P(W_n|W_{n-1})\\
	\end{aligned}
\label{ngram_approx}
\end{equation}


本文采用的是\textbf{jieba分词}的分词词工具，结巴分词主要是采用
\begin{itemize}
	\item 基于前缀的词典实现的词图扫描，对句子中的汉字的所有成词情况，生成有向无环图 (DAG)
	\item 通过动态规划来查找最大概率路径, 找到基于词频的最大切分组合
	\item 采用了HMM模型，并使用$Viterbi$算法算法优化，解决了未登陆词的问题
\end{itemize}

测试数据表明，jieba分词能正确地切分绝大多的句子，即使句子中有未登陆词，jieba分词也能正确地切分出来，在处理一些极端的情况，如“结婚的和尚未结婚的”这种句子，jieba分词也能正确地切分。


\subsection{Word2Vec词向量化}
经过了jieba分词的处理，我们已经把每个句子转化为词语组成的集合，但是聚类算法主要是通过计算两个向量的距离来判断两个样本的相似度，所以我们光拿词语组成的集合是不够的。还需要把词语转换成向量。

把句子转化为向量的方法大致有两种，1.通过词袋模型转换，2. 通过词向量模型转换


\subsubsection{词袋模型}
词袋模型，顾名思义也就是将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的。
词袋模型最常用的方法是One-Hot Encoding，即把数据中所有词语去重后，一一映射到一个大小与去重词语数量相等的数组上，数组的每元素值都只能为0或者1，0表示映射到该下标的词语没有出现，1表示映射到该下标的词语出现了。具体例子如下

\newpage
首先给出两个简单的样本，如下所示: 

\begin{enumerate}
	\item $[$ He, is, a, male $]$
	\item $[$ She, is, a, female $]$
\end{enumerate}

根据上面两个例句，就可以构成一个词袋，袋子中含有的词为He、She、is、a、male、female，假设构建一个数组[He, She, is, a, male, female]来匹配映射，那么上面两句话可以转化成以下两组向量来表示

\begin{enumerate}
	\item $[$ 1, 0, 1, 1, 1, 0 $]$
	\item $[$ 0, 1, 1, 1, 0, 1 $]$
\end{enumerate}
	
这两个词频向量就是基于词袋模型转换出来的向量，可以看出，这样转换的坏处是语境上下文完全丢失了，只保留了单词出现的频率，这是不足以完全展示一个句子的含义。

同时，当你的词袋内的词越多，向量的纬度就会越大。我在一开始做词向量化的时候用的就是词袋模型，导致我的向量的纬度是十几万！这运行起来简直是个灾难，速度非常慢。所以一般我们不用原生的词袋模型来做词向量化。

\subsubsection{词向量模型}

    词向量模型是考虑词语位置关系的一种模型。通过大量语料的训练，将每一个词语映射到高维度（几千、几万维以上）的向量当中，通过求余弦的方式，可以判断两个词语之间的关系，例如例句中的Jane和Bob在词向量模型中，他们的余弦值可能就接近1，因为这两个都是人名，Shenzhen和Bob的余弦值可能就接近0，因为一个是人名一个是地名。

    现在常用word2vec构成词向量模型，它的底层采用基于CBOW和Skip-Gram算法的神经网络模型。

1. CBOW模型

    CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。比如上面的第一句话，将上下文大小取值为2，特定的这个词是"go"，也就是我们需要的输出词向量,上下文对应的词有4个，前后各2个，这4个词是我们模型的输入。由于CBOW使用的是词袋模型，因此这4个词都是平等的，也就是不考虑他们和我们关注的词之间的距离大小，只要在我们上下文之内即可。

    这样我们这个CBOW的例子里，我们的输入是4个词向量，输出是所有词的softmax概率（训练的目标是期望训练样本特定词对应的softmax概率最大），对应的CBOW神经网络模型输入层有4个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某4个词对应的最可能的输出中心词时，我们可以通过一次DNN前向传播算法并通过softmax激活函数找到概率最大的词对应的神经元即可。


\begin{figure}[htbp]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{cbow.png}
		\caption{CBOW 模型}
		\label{word_vec:cbow}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{Skip-gram.png}
		\caption{Skip-Gram 模型}
		\label{word_vec:skip_gram}
	\end{subfigure}
	\caption{词向量模型}
	\label{word_vec:example}
\end{figure}

2.Skip-Gram模型

    Skip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。还是上面的例子，我们的上下文大小取值为2， 特定的这个词"go"是我们的输入，而这4个上下文词是我们的输出。

    这样我们这个Skip-Gram的例子里，我们的输入是特定词， 输出是softmax概率排前4的4个词，对应的Skip-Gram神经网络模型输入层有1个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某1个词对应的最可能的4个上下文词时，我们可以通过一次DNN前向传播算法得到概率大小排前4的softmax概率对应的神经元所对应的词即可。

词向量模型突出特点：

    在词向量模型中，词向量与词向量之间有这非常特殊的特性。例如现在存在国王、男生、女人、皇后四个词向量，那么一个完善的词向量模型，就存在“国王-男人+女人=皇后”这样的关系。
