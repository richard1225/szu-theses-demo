\section{引言}

\subsection{研究背景及意义}
随着社会的进步与发展，以及信息逐渐趋向透明、公开化，越来越多案件的审判过程与结果暴露在公众的视野下，接受着公众的监督。然而，在同一件案子下，不同的法官可能会得出不同的审判结果，这是因为每个人都有自己的评判标准。在此情形下，让案件得到公正的审判、减少个人尺度标准差异带来的影响，变得尤为重要。通过本论文提出的方法，能够训练出一个模型，该模型可以从当前判决书中提取出关键因子，再通过多项关键因子推荐相似的已判决过的案件，这些案件给法官作为一个参考，以减少个人因素带来的影响，除此之外，此技术还能帮助减轻法官，律师的工作量，让他们能把精力更多地投入到案子。这能带来一个更公正的审判结果，从而引导一个和谐的社会舆论氛围。

随着最高人民法院对审判流程信息、裁判文书信息、执行信息全面公开的推进，中国裁判文书网在此背景下诞生，作为全国法院统一的裁判文书公开平台，裁判文书生效后七日内都将被传送至中国裁判文书网公布累计只2017年的统计，库内判决书已达3247万篇\cite{WEB:judgement}，这繁杂的信息中，有很多信息对律师法官，乃至人民群众来说，都有很高的参考价值，如何利用好这么大的数据，就要使用到自然语言处理技术(NLP)

律师与法官在接受到一个案子的时候，首先要做的就是从案件中提取关键点。关键点就是这个案子的争议焦点、历史判决、双方诉称辩称、引用的法律发条等等。然后，根据这些关键点，去裁判文书网中搜索类似的案件作为参考。但是这样检索存在两个问题，一是关键点通常都是一句陈述句，并且在不同案件下，同一个意思往往有好几种表达方法，这类关键句在裁判文书网中往往检索不到想要的结果。二是检索到的结果都是弱相关的，仍然需要用户进一步自行筛选。这样的流程得出来的结果是低效，精准度差的。

得益于数据挖掘中的文本挖掘的发展，以上痛点均可通过自然语言处理技术来解决。首先从裁判文书网获取若干判决书作为训练姐，再对训练集进行中文预处理，以句来切分，对所有的句子使用Kmeans聚类算法来聚类，从聚类结果中挑选出符合关键因子特征的簇，作为一类关键因子。通过反复的迭代与挑选，可以选出足够数量的簇，足以覆盖绝大多数的关键点。再用这些簇来训练出一个分类模型，即可得出一个能从新的判决书中分类出关键句子的模型。在这个过程中，聚类的质量直接影响了分类模型的精准度。本人在此项目中主要从事聚类相关工作，因此本文主是要围绕聚类算法来展开研究，对比多种聚类算法的质量。

\subsection{国内外研究现状}
虽然很早之前，就有人开始研究自然语言处理，但真正提出用计算机来处理自然语言，是在1950年。当时，艾伦图灵提出了一个标准，意为能通过”图灵测试“\cite{WEB:turing_test}标准的程序可以被判断为是智能的。概念提出之后一直到1980年代，人们研发的NLP系统都是基于规则的，由复杂的规则堆砌而成，那时候的人们调侃这种系统为“积木系统”，对于超过规则之外的输入，这种系统只能给出机械式的，无关痛痒的回答。1980年之后，人们把机器学习引入了NLP来提升它的准确性和稳定性。除此之，NLP还因为两个革新效果大幅度提升，一是运算能力稳定增加（硬件的性能和价格遵守摩尔定律），二是转换-生成文法方法的乔姆斯基语言学理论不再是NLP的主要方法\cite{WEB:turing_test}。从此之后，各领域的NLP技术稳步发展，逐渐走向成熟。

近期，已经有人开始把深度学习的技巧也引入NLP，在自然语言处理方面取得了巨大的成就，如Yonghui Wu等人发表的Exploring the Limits of Language Modeling\cite{DBLP:journals/corr/JozefowiczVSSW16}。但本文只研究基于无监督学习的聚类算法。

作为NLP的重要分析手段，聚类分析，是当下的一个热门研究对象。在聚类分析的发展进程中，产生了大量的聚类算法，主流的为以下几种：中心聚类、层次聚类、密度聚类、谱聚类等。

聚类算法的目的，通俗地来讲，是将具有共性的样本归类为统一类别，再将少有甚至没有共性的样本归类为不同类别。这里的共性通常是通过度量样本之间的距离来求得的，对距离的定义有很多种，我们需要结合样本的特点来具体分析。\cite{ZW:cluster_alg_study_compare}

总之，聚类算法最终是要得到一批符合以下亮点特征的簇的集合：一是每个簇（又称为类簇）内的距离相对紧密，二是不同簇的簇间距离相对较大。

\subsection{研究目的与主要工作}
本文主要研究的聚类算法是KMeans\cite{Macqueen67somemethods}算法和GSDMM\cite{Yin:2014:DMM:2623330.2623715}算法

\subsubsection{KMeans算法}

KMeans算法的主要思想是寻找每个簇的中心，再把与中心具有共性的点归类的该中心所代表的类簇中。KMeans算法是迭代执行的，需要给出有一个参数K，K为预估的类簇的数量。初始的时候，会随机生产K个中心点，在KMeans的每轮迭代中，都会更新K个中心点的位置，重复迭代，直到K个中心点的位置不再变化，此时可以判断为收敛了，所得的K个点的集合，经过筛选之后，即可选为最终的类簇。用KMeans算法的好处是收敛速度比较快，因为其计算比较简单，是通过不断计算距离来迭代更新的，这个距离一般是欧式距离。

\subsubsection{GSDMM算法}
GSDMM算法是一个基于Dirichlet多项式混合模型的算法，发表自清华大学信息科学与技术国家实验室的尹建华和尹建勇\cite{Yin:2014:DMM:2623330.2623715}，其算法给出了一个通俗易懂的例子来理解这个算法：一位教授正在教一个电影课。在课程开始时，学生被随机分配到K个桌子。在课程开始前，学生们列出一个他们最喜欢的电影的表单。教授则反复阅读每一位学生的表单。每次叫到一位学生的时候，这位学生会被分配到至少满足以下一个条件的新桌子去：
\begin{enumerate}
	\item 新桌子的学生比当前学生所在的桌子的学生多
	\item 新桌子的学生的电影清单都是相似的
\end{enumerate}
通过不断地迭代，预期所有的学生都会被分配到“最优”的桌子去。GSDMM算法的好处是无需指定K值，该算法会自动调整K值，并且根据作者的测试结果，GSDMM的耗时比KMeans少了4/5。

\subsection{本章小结}

本章主要介绍了相关需求背景和聚类算法的发展历史，对KMeans算法和GSDMM算法进行了粗浅的初步解释，在第三章中我会更加细致地解释这两种算法。
